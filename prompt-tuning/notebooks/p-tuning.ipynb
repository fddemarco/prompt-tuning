{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc91e6f-f9a0-4e2e-850a-79d8fa2ebdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200cbb41-f75c-43bc-835a-738f3739f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"roberta-large\"\n",
    "task = \"mrpc\"\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e0497-8372-410a-bad5-3661ed460485",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_dataset = datasets.load_dataset(\"glue\", task)\n",
    "glue_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c7198-3673-461b-9704-8206ba34d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"glue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071217b2-3dc0-4eac-a5b6-7f06971806d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    print(predictions)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786d9ae-ed40-4278-9ca0-11c986c6b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cd88c-6969-4bc9-9fc7-fad03992984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = glue_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11516c27-009d-44e3-a39a-afebc23d6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa2c06-ad29-4ca6-8273-8ba6440412d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = peft.PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20, encoder_hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d085460-800f-41b7-a105-0aab8b12ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
    "model = peft.get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f19bbf-03cd-4216-8fe3-4f3ff56002ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"your-name/roberta-large-peft-p-tuning\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5bed0-056c-4750-8fbe-97d73a5d3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204483b7-b96d-49b1-b748-bc8baa1daa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"smangrul/roberta-large-peft-p-tuning\"\n",
    "config = peft.PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = transformers.AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = peft.PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b663f3-d8ac-4127-9211-edc20feeedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"not equivalent\", \"equivalent\"]\n",
    "\n",
    "sentence1 = \"Coast redwood trees are the tallest trees on the planet and can grow over 300 feet tall.\"\n",
    "sentence2 = \"The coast redwood trees, which can attain a height of over 300 feet, are the tallest trees on earth.\"\n",
    "\n",
    "inputs = tokenizer(sentence1, sentence2, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8afa430-6b28-463a-8525-035a6628a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "    print(outputs)\n",
    "\n",
    "paraphrased_text = torch.softmax(outputs, dim=1).tolist()[0]\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrased_text[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a661bd0-2bef-4358-a3b5-c37038a297a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
